{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c1d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8eda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed6269b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your name:hamesh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def start_capture(name):\n",
    "        path = \"/Users/prakhar/Desktop/frs/Face-Images/Final-Training-Images\" + name\n",
    "        num_of_images = 0\n",
    "        detector = cv2.CascadeClassifier(\"/Users/prakhar/Desktop/frs/haarcascade_frontalface_default.xml\")\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except:\n",
    "            print('Directory Already Created')\n",
    "        vid = cv2.VideoCapture(0)\n",
    "        while True:\n",
    "\n",
    "            ret, img = vid.read()\n",
    "            new_img = None\n",
    "            grayimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            face = detector.detectMultiScale(image=grayimg, scaleFactor=1.1, minNeighbors=5)\n",
    "            for x, y, w, h in face:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 0), 2)\n",
    "                cv2.putText(img, \"Face Detected\", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255))\n",
    "                cv2.putText(img, str(str(num_of_images)+\" images captured\"), (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255))\n",
    "                new_img = img[y:y+h, x:x+w]\n",
    "            cv2.imshow(\"FaceDetection\", img)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\n",
    "            try :\n",
    "                cv2.imwrite(str(path+\"/\"+str(num_of_images)+name+\".jpg\"), new_img)\n",
    "                num_of_images += 1\n",
    "            except :\n",
    "\n",
    "                pass\n",
    "            if key == ord(\"q\") or key == 27 or num_of_images > 100:\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "        return num_of_images\n",
    "    \n",
    "name=input(\"enter your name:\")\n",
    "start_capture(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4a4c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 774 images belonging to 121 classes.\n",
      "Found 774 images belonging to 121 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AJ_Cook': 0,\n",
       " 'AJ_Lamas': 1,\n",
       " 'Aaron_Eckhart': 2,\n",
       " 'Aaron_Guiel': 3,\n",
       " 'Aaron_Patterson': 4,\n",
       " 'Aaron_Peirsol': 5,\n",
       " 'Aaron_Pena': 6,\n",
       " 'Aaron_Sorkin': 7,\n",
       " 'Aaron_Tippin': 8,\n",
       " 'Abba_Eban': 9,\n",
       " 'Abbas_Kiarostami': 10,\n",
       " 'Abdel_Aziz_Al-Hakim': 11,\n",
       " 'Abdel_Madi_Shabneh': 12,\n",
       " 'Abdel_Nasser_Assidi': 13,\n",
       " 'Abdoulaye_Wade': 14,\n",
       " 'Abdul_Majeed_Shobokshi': 15,\n",
       " 'Abdul_Rahman': 16,\n",
       " 'Abdulaziz_Kamilov': 17,\n",
       " 'Abdullah': 18,\n",
       " 'Abdullah_Ahmad_Badawi': 19,\n",
       " 'Abdullah_Gul': 20,\n",
       " 'Abdullah_Nasseef': 21,\n",
       " 'Abdullah_al-Attiyah': 22,\n",
       " 'Abdullatif_Sener': 23,\n",
       " 'Abel_Aguilar': 24,\n",
       " 'Abel_Pacheco': 25,\n",
       " 'Abid_Hamid_Mahmud_Al-Tikriti': 26,\n",
       " 'Abner_Martinez': 27,\n",
       " 'Abraham_Foxman': 28,\n",
       " 'Aby_Har-Even': 29,\n",
       " 'Adam_Ant': 30,\n",
       " 'Adam_Freier': 31,\n",
       " 'Adam_Herbert': 32,\n",
       " 'Adam_Kennedy': 33,\n",
       " 'Adam_Mair': 34,\n",
       " 'Adam_Rich': 35,\n",
       " 'Adam_Sandler': 36,\n",
       " 'Adam_Scott': 37,\n",
       " 'Adel_Al-Jubeir': 38,\n",
       " 'Adelina_Avila': 39,\n",
       " 'Adisai_Bodharamik': 40,\n",
       " 'Adolfo_Aguilar_Zinser': 41,\n",
       " 'Adolfo_Rodriguez_Saa': 42,\n",
       " 'Adoor_Gopalakarishnan': 43,\n",
       " 'Adrian_Annus': 44,\n",
       " 'Adrian_Fernandez': 45,\n",
       " 'Adrian_McPherson': 46,\n",
       " 'Adrian_Murrell': 47,\n",
       " 'Adrian_Nastase': 48,\n",
       " 'Adriana_Lima': 49,\n",
       " 'Adriana_Perez_Navarro': 50,\n",
       " 'Adrianna_Zuzic': 51,\n",
       " 'Adrien_Brody': 52,\n",
       " 'Afton_Smith': 53,\n",
       " 'Agbani_Darego': 54,\n",
       " 'Agnelo_Queiroz': 55,\n",
       " 'Agnes_Bruckner': 56,\n",
       " 'Ahmad_Jbarah': 57,\n",
       " 'Ahmad_Masood': 58,\n",
       " 'Ahmed_Ahmed': 59,\n",
       " 'Ahmed_Chalabi': 60,\n",
       " 'Ahmed_Ghazi': 61,\n",
       " 'Ahmed_Ibrahim_Bilal': 62,\n",
       " 'Ahmed_Lopez': 63,\n",
       " 'Ahmed_Qureia': 64,\n",
       " 'Ahmet_Demir': 65,\n",
       " 'Ahmet_Necdet_Sezer': 66,\n",
       " 'Ai_Sugiyama': 67,\n",
       " 'Aicha_El_Ouafi': 68,\n",
       " 'Aidan_Quinn': 69,\n",
       " 'Aileen_Riggin_Soule': 70,\n",
       " 'Ain_Seppik': 71,\n",
       " 'Ainsworth_Dyer': 72,\n",
       " 'Aishwarya_Rai': 73,\n",
       " 'Aitor_Gonzalez': 74,\n",
       " 'Aiysha_Smith': 75,\n",
       " 'Ajit_Agarkar': 76,\n",
       " 'Akbar_Al_Baker': 77,\n",
       " 'Akbar_Hashemi_Rafsanjani': 78,\n",
       " 'Akhmed_Zakayev': 79,\n",
       " 'Akiko_Morigami': 80,\n",
       " 'Akmal_Taher': 81,\n",
       " 'Al_Cardenas': 82,\n",
       " 'Al_Davis': 83,\n",
       " 'Al_Gore': 84,\n",
       " 'Al_Leiter': 85,\n",
       " 'Al_Pacino': 86,\n",
       " 'Al_Sharpton': 87,\n",
       " 'Alain_Cervantes': 88,\n",
       " 'Alain_Ducasse': 89,\n",
       " 'Alan_Ball': 90,\n",
       " 'Alan_Dershowitz': 91,\n",
       " 'Alan_Dreher': 92,\n",
       " 'Alan_Greenspan': 93,\n",
       " 'Alan_Greer': 94,\n",
       " 'Alan_Jackson': 95,\n",
       " 'Alan_Mulally': 96,\n",
       " 'Alan_Stonecipher': 97,\n",
       " 'Alan_Tang_Kwong-wing': 98,\n",
       " 'Alan_Trammell': 99,\n",
       " 'Harsha': 100,\n",
       " 'Jeet': 101,\n",
       " 'Prakhar': 102,\n",
       " 'face1': 103,\n",
       " 'face10': 104,\n",
       " 'face11': 105,\n",
       " 'face12': 106,\n",
       " 'face13': 107,\n",
       " 'face14': 108,\n",
       " 'face15': 109,\n",
       " 'face16': 110,\n",
       " 'face17': 111,\n",
       " 'face18': 112,\n",
       " 'face2': 113,\n",
       " 'face3': 114,\n",
       " 'face4': 115,\n",
       " 'face5': 116,\n",
       " 'face6': 117,\n",
       " 'face7': 118,\n",
       " 'face8': 119,\n",
       " 'face9': 120}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    " \n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    " \n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath='/Users/prakhar/Desktop/frs/Face-Images/Final-Training-Images'\n",
    " \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    " \n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    " \n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()\n",
    " \n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    " \n",
    " \n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    " \n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fd0cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'AJ_Cook', 1: 'AJ_Lamas', 2: 'Aaron_Eckhart', 3: 'Aaron_Guiel', 4: 'Aaron_Patterson', 5: 'Aaron_Peirsol', 6: 'Aaron_Pena', 7: 'Aaron_Sorkin', 8: 'Aaron_Tippin', 9: 'Abba_Eban', 10: 'Abbas_Kiarostami', 11: 'Abdel_Aziz_Al-Hakim', 12: 'Abdel_Madi_Shabneh', 13: 'Abdel_Nasser_Assidi', 14: 'Abdoulaye_Wade', 15: 'Abdul_Majeed_Shobokshi', 16: 'Abdul_Rahman', 17: 'Abdulaziz_Kamilov', 18: 'Abdullah', 19: 'Abdullah_Ahmad_Badawi', 20: 'Abdullah_Gul', 21: 'Abdullah_Nasseef', 22: 'Abdullah_al-Attiyah', 23: 'Abdullatif_Sener', 24: 'Abel_Aguilar', 25: 'Abel_Pacheco', 26: 'Abid_Hamid_Mahmud_Al-Tikriti', 27: 'Abner_Martinez', 28: 'Abraham_Foxman', 29: 'Aby_Har-Even', 30: 'Adam_Ant', 31: 'Adam_Freier', 32: 'Adam_Herbert', 33: 'Adam_Kennedy', 34: 'Adam_Mair', 35: 'Adam_Rich', 36: 'Adam_Sandler', 37: 'Adam_Scott', 38: 'Adel_Al-Jubeir', 39: 'Adelina_Avila', 40: 'Adisai_Bodharamik', 41: 'Adolfo_Aguilar_Zinser', 42: 'Adolfo_Rodriguez_Saa', 43: 'Adoor_Gopalakarishnan', 44: 'Adrian_Annus', 45: 'Adrian_Fernandez', 46: 'Adrian_McPherson', 47: 'Adrian_Murrell', 48: 'Adrian_Nastase', 49: 'Adriana_Lima', 50: 'Adriana_Perez_Navarro', 51: 'Adrianna_Zuzic', 52: 'Adrien_Brody', 53: 'Afton_Smith', 54: 'Agbani_Darego', 55: 'Agnelo_Queiroz', 56: 'Agnes_Bruckner', 57: 'Ahmad_Jbarah', 58: 'Ahmad_Masood', 59: 'Ahmed_Ahmed', 60: 'Ahmed_Chalabi', 61: 'Ahmed_Ghazi', 62: 'Ahmed_Ibrahim_Bilal', 63: 'Ahmed_Lopez', 64: 'Ahmed_Qureia', 65: 'Ahmet_Demir', 66: 'Ahmet_Necdet_Sezer', 67: 'Ai_Sugiyama', 68: 'Aicha_El_Ouafi', 69: 'Aidan_Quinn', 70: 'Aileen_Riggin_Soule', 71: 'Ain_Seppik', 72: 'Ainsworth_Dyer', 73: 'Aishwarya_Rai', 74: 'Aitor_Gonzalez', 75: 'Aiysha_Smith', 76: 'Ajit_Agarkar', 77: 'Akbar_Al_Baker', 78: 'Akbar_Hashemi_Rafsanjani', 79: 'Akhmed_Zakayev', 80: 'Akiko_Morigami', 81: 'Akmal_Taher', 82: 'Al_Cardenas', 83: 'Al_Davis', 84: 'Al_Gore', 85: 'Al_Leiter', 86: 'Al_Pacino', 87: 'Al_Sharpton', 88: 'Alain_Cervantes', 89: 'Alain_Ducasse', 90: 'Alan_Ball', 91: 'Alan_Dershowitz', 92: 'Alan_Dreher', 93: 'Alan_Greenspan', 94: 'Alan_Greer', 95: 'Alan_Jackson', 96: 'Alan_Mulally', 97: 'Alan_Stonecipher', 98: 'Alan_Tang_Kwong-wing', 99: 'Alan_Trammell', 100: 'Harsha', 101: 'Jeet', 102: 'Prakhar', 103: 'face1', 104: 'face10', 105: 'face11', 106: 'face12', 107: 'face13', 108: 'face14', 109: 'face15', 110: 'face16', 111: 'face17', 112: 'face18', 113: 'face2', 114: 'face3', 115: 'face4', 116: 'face5', 117: 'face6', 118: 'face7', 119: 'face8', 120: 'face9'}\n",
      "\n",
      " The Number of output neurons:  121\n"
     ]
    }
   ],
   "source": [
    "'''############ Creating lookup table for all faces ############'''\n",
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    " \n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    " \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    " \n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2629c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "25/25 [==============================] - 9s 307ms/step - loss: 12.3728 - accuracy: 0.1240 - val_loss: 3.3267 - val_accuracy: 0.2156\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 6s 237ms/step - loss: 2.8491 - accuracy: 0.3915 - val_loss: 2.4255 - val_accuracy: 0.4469\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 2.2547 - accuracy: 0.4974 - val_loss: 1.8337 - val_accuracy: 0.5875\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 1.7520 - accuracy: 0.6150 - val_loss: 1.6002 - val_accuracy: 0.6344\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 6s 229ms/step - loss: 1.4922 - accuracy: 0.6654 - val_loss: 1.2933 - val_accuracy: 0.7156\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 6s 229ms/step - loss: 1.3835 - accuracy: 0.6925 - val_loss: 1.3121 - val_accuracy: 0.7094\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 1.2065 - accuracy: 0.7209 - val_loss: 1.0025 - val_accuracy: 0.7812\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 1.1357 - accuracy: 0.7403 - val_loss: 0.9171 - val_accuracy: 0.8000\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 1.0476 - accuracy: 0.7597 - val_loss: 0.7893 - val_accuracy: 0.8219\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 6s 229ms/step - loss: 0.9022 - accuracy: 0.7817 - val_loss: 0.7709 - val_accuracy: 0.7812\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 6s 231ms/step - loss: 0.8528 - accuracy: 0.7842 - val_loss: 0.6410 - val_accuracy: 0.8250\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 6s 238ms/step - loss: 0.7948 - accuracy: 0.7972 - val_loss: 0.6887 - val_accuracy: 0.8281\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 6s 231ms/step - loss: 0.7031 - accuracy: 0.8269 - val_loss: 0.4730 - val_accuracy: 0.8656\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.7948 - accuracy: 0.7984 - val_loss: 0.5948 - val_accuracy: 0.8438\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.7148 - accuracy: 0.8152 - val_loss: 0.5797 - val_accuracy: 0.8344\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.6765 - accuracy: 0.8282 - val_loss: 0.4681 - val_accuracy: 0.8625\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.5760 - accuracy: 0.8437 - val_loss: 0.3573 - val_accuracy: 0.8938\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.4667 - accuracy: 0.8695 - val_loss: 0.2964 - val_accuracy: 0.8938\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.4769 - accuracy: 0.8579 - val_loss: 0.3776 - val_accuracy: 0.8813\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 0.5006 - accuracy: 0.8527 - val_loss: 0.3663 - val_accuracy: 0.9031\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.4384 - accuracy: 0.8760 - val_loss: 0.2739 - val_accuracy: 0.9156\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.3317 - accuracy: 0.9109 - val_loss: 0.1852 - val_accuracy: 0.9344\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 6s 236ms/step - loss: 0.3636 - accuracy: 0.8902 - val_loss: 0.2567 - val_accuracy: 0.9125\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.3169 - accuracy: 0.8966 - val_loss: 0.1732 - val_accuracy: 0.9344\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 0.3302 - accuracy: 0.9070 - val_loss: 0.2341 - val_accuracy: 0.9281\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 6s 231ms/step - loss: 0.3016 - accuracy: 0.9070 - val_loss: 0.1411 - val_accuracy: 0.9625\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 6s 238ms/step - loss: 0.2259 - accuracy: 0.9289 - val_loss: 0.1420 - val_accuracy: 0.9438\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.2476 - accuracy: 0.9199 - val_loss: 0.1243 - val_accuracy: 0.9594\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 6s 233ms/step - loss: 0.2041 - accuracy: 0.9315 - val_loss: 0.1409 - val_accuracy: 0.9344\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 6s 236ms/step - loss: 0.2114 - accuracy: 0.9276 - val_loss: 0.1189 - val_accuracy: 0.9563\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 6s 230ms/step - loss: 0.2063 - accuracy: 0.9264 - val_loss: 0.1659 - val_accuracy: 0.9469\n",
      "Epoch 32/40\n",
      "25/25 [==============================] - 6s 234ms/step - loss: 0.1876 - accuracy: 0.9380 - val_loss: 0.0705 - val_accuracy: 0.9750\n",
      "Epoch 33/40\n",
      "25/25 [==============================] - 6s 252ms/step - loss: 0.2133 - accuracy: 0.9302 - val_loss: 0.1000 - val_accuracy: 0.9625\n",
      "Epoch 34/40\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.2033 - accuracy: 0.9367 - val_loss: 0.0972 - val_accuracy: 0.9688\n",
      "Epoch 35/40\n",
      "25/25 [==============================] - 6s 232ms/step - loss: 0.1776 - accuracy: 0.9276 - val_loss: 0.1115 - val_accuracy: 0.9500\n",
      "Epoch 36/40\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.1783 - accuracy: 0.9444 - val_loss: 0.0876 - val_accuracy: 0.9625\n",
      "Epoch 37/40\n",
      "25/25 [==============================] - 6s 230ms/step - loss: 0.1448 - accuracy: 0.9444 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 38/40\n",
      "25/25 [==============================] - 6s 227ms/step - loss: 0.1184 - accuracy: 0.9470 - val_loss: 0.0640 - val_accuracy: 0.9781\n",
      "Epoch 39/40\n",
      "25/25 [==============================] - 819s 34s/step - loss: 0.1036 - accuracy: 0.9561 - val_loss: 0.0522 - val_accuracy: 0.9844\n",
      "Epoch 40/40\n",
      "25/25 [==============================] - 7s 287ms/step - loss: 0.1278 - accuracy: 0.9509 - val_loss: 0.0751 - val_accuracy: 0.9750\n",
      "###### Total Time Taken:  18 Minutes ######\n"
     ]
    }
   ],
   "source": [
    "'''######################## Create CNN deep learning model ########################'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
    " \n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(121, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " \n",
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "\n",
    "num_epochs = 40\n",
    "classifier.fit(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=25,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1689537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  Jeet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''########### Making single predictions ###########'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    " \n",
    "ImagePath='/Users/prakhar/Desktop/frs/Face-Images/Final-Testing-Images/Harsha/28Jeet.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21b8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  Harsha\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''########### Making single predictions ###########'''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    " \n",
    "ImagePath='/Users/prakhar/Desktop/frs/Face-Images/Final-Testing-Images/Harsha/15Harsha.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006d306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
